# üöÄ LobeChat + Sua API - Quick Start

## ‚ö° In√≠cio R√°pido (3 minutos)

### 1. Inicie sua API

```bash
cd /home/vdilinux/aplica√ß√µes/api-adk-google-main
./scripts/start_backend.sh
```

‚úÖ API rodando em: `http://localhost:8001`

---

### 2. Configure o LobeChat

Abra o LobeChat e configure conforme a imagem que voc√™ mostrou:

#### No campo "Chave de API personalizada":
```
your-api-key-here
```
(Pode ser qualquer valor por enquanto)

#### No campo "URL Base" (se dispon√≠vel):
```
http://localhost:8001/v1
```

---

### 3. Teste!

Envie uma mensagem no LobeChat:
```
Ol√°! Como voc√™ est√°?
```

‚úÖ Deve funcionar! O LobeChat vai usar sua API como backend.

---

## üîß Configura√ß√£o Detalhada

### Se o LobeChat estiver online (https://lobehub.com)

**‚ö†Ô∏è Importante:** Para o LobeChat online acessar sua API local, voc√™ precisa expor sua API para a internet.

#### Op√ß√£o 1: Ngrok (Mais R√°pido)

```bash
# 1. Instale o ngrok
sudo snap install ngrok

# 2. Autentique (crie conta em https://ngrok.com)
ngrok authtoken YOUR_TOKEN

# 3. Exponha sua API
ngrok http 8001
```

Copie a URL gerada (ex: `https://abc123.ngrok.io`) e use no LobeChat:

```
Base URL: https://abc123.ngrok.io/v1
API Key: your-api-key-here
```

---

### Se voc√™ quer hospedar o LobeChat localmente

#### Docker (Recomendado)

```bash
docker run -d \
  --name lobechat \
  -p 3210:3210 \
  -e OPENAI_API_KEY=your-api-key-here \
  -e API_BASE_URL=http://host.docker.internal:8001/v1 \
  lobehub/lobe-chat:latest
```

Acesse: `http://localhost:3210`

---

## üìã Modelos Dispon√≠veis

Sua API suporta estes modelos via LiteLLM:

### Google Gemini
```
gemini/gemini-2.0-flash-exp
gemini/gemini-2.5-flash
gemini/gemini-1.5-pro
gemini/gemini-1.5-flash
```

### OpenAI
```
openai/gpt-4o
openai/gpt-4o-mini
openai/gpt-4-turbo
openai/gpt-3.5-turbo
```

### Anthropic Claude
```
anthropic/claude-3-opus-20240229
anthropic/claude-3-sonnet-20240229
anthropic/claude-3-haiku-20240307
```

### Ollama (Local)
```
ollama/llama2
ollama/llama3
ollama/mistral
ollama/codellama
```

---

## üß™ Testar API Manualmente

### Listar Modelos

```bash
curl -X GET http://localhost:8001/v1/models \
  -H "Authorization: Bearer test-key"
```

### Chat

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Authorization: Bearer test-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini/gemini-2.0-flash-exp",
    "messages": [
      {"role": "user", "content": "Ol√°!"}
    ]
  }'
```

---

## üêõ Problemas Comuns

### "Connection Refused"

**Solu√ß√£o:** Verifique se a API est√° rodando:
```bash
curl http://localhost:8001/health
```

### "Invalid API Key"

**Solu√ß√£o:** Use qualquer valor como API key:
```
Authorization: Bearer your-api-key-here
```

### "Model not found"

**Solu√ß√£o:** Use o formato correto:
- ‚úÖ `gemini/gemini-2.0-flash-exp`
- ‚ùå `gemini-2.0-flash-exp`

### LobeChat n√£o conecta

**Se LobeChat est√° no Docker:**
- Use `http://host.docker.internal:8001/v1` em vez de `http://localhost:8001/v1`

**Se LobeChat est√° online:**
- Use ngrok ou cloudflare tunnel para expor sua API com HTTPS

---

## üìö Documenta√ß√£o Completa

Para configura√ß√£o avan√ßada, consulte:
- [`docs/LOBECHAT_INTEGRATION.md`](./docs/LOBECHAT_INTEGRATION.md) - Guia completo
- [`docs/arquitetura/litellm/`](./docs/arquitetura/litellm/) - Documenta√ß√£o LiteLLM

---

## ‚úÖ Checklist

- [ ] API est√° rodando (`./scripts/start_backend.sh`)
- [ ] Configurou o LobeChat com a Base URL correta
- [ ] Configurou uma API Key (qualquer valor)
- [ ] Testou enviando uma mensagem
- [ ] Funciona! üéâ

---

**D√∫vidas?** Consulte `docs/LOBECHAT_INTEGRATION.md` para troubleshooting detalhado.

**√öltima atualiza√ß√£o:** 2025-11-12

