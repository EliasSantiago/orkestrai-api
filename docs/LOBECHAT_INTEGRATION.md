# üé® Integra√ß√£o com LobeChat

## üìã Vis√£o Geral

Este guia mostra como usar sua **API FastAPI com LiteLLM** como backend para o **LobeChat**.

**O que foi implementado:**
- ‚úÖ Endpoints compat√≠veis com OpenAI API (`/v1/models`, `/v1/chat/completions`)
- ‚úÖ Suporte a streaming e non-streaming
- ‚úÖ Roteamento autom√°tico via LiteLLM para 100+ modelos
- ‚úÖ Autentica√ß√£o via Bearer token

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             ‚îÇ         ‚îÇ                  ‚îÇ         ‚îÇ                 ‚îÇ
‚îÇ  LobeChat   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ  Sua API FastAPI ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ    LiteLLM      ‚îÇ
‚îÇ  (Frontend) ‚îÇ         ‚îÇ  (Backend/Proxy) ‚îÇ         ‚îÇ    (Roteador)   ‚îÇ
‚îÇ             ‚îÇ         ‚îÇ                  ‚îÇ         ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                        ‚îÇ                             ‚îÇ
      ‚îÇ                        ‚îÇ                             ‚îÇ
      ‚îÇ                        ‚îÇ                             ‚îú‚îÄ> Gemini
      ‚îÇ                        ‚îÇ                             ‚îú‚îÄ> OpenAI
      ‚îÇ                        ‚îÇ                             ‚îú‚îÄ> Claude
      ‚îÇ                        ‚îÇ                             ‚îú‚îÄ> Ollama
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ> Azure
                                                              
```

**Fluxo:**
1. LobeChat envia requisi√ß√£o para `/v1/chat/completions`
2. Sua API valida autentica√ß√£o
3. LiteLLM roteia para o modelo correto
4. Resposta √© enviada de volta ao LobeChat

---

## üöÄ Passo a Passo

### 1. Certifique-se de que sua API est√° rodando

```bash
# Inicie o servidor
./scripts/start_backend.sh
```

Sua API estar√° dispon√≠vel em: `http://localhost:8001`

### 2. Endpoints OpenAI-Compatible Dispon√≠veis

#### **GET /v1/models** - Listar modelos

```bash
curl -X GET http://localhost:8001/v1/models \
  -H "Authorization: Bearer your-api-key-here"
```

**Resposta:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "gemini/gemini-2.0-flash-exp",
      "object": "model",
      "created": 1699500000,
      "owned_by": "litellm"
    },
    {
      "id": "openai/gpt-4o",
      "object": "model",
      "created": 1699500000,
      "owned_by": "litellm"
    }
    // ... mais modelos
  ]
}
```

#### **POST /v1/chat/completions** - Chat

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Authorization: Bearer your-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini/gemini-2.0-flash-exp",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    "stream": true
  }'
```

---

## üé® Configurar LobeChat

### Op√ß√£o 1: Usando LobeChat Hospedado (Mais F√°cil)

Se voc√™ est√° usando o LobeChat hospedado em https://lobehub.com ou outra inst√¢ncia online:

1. **Acesse as Configura√ß√µes** no LobeChat

2. **V√° para "Configura√ß√µes de Modelo"** ou "Model Provider"

3. **Selecione "Custom OpenAI"** ou "OpenAI Compatible"

4. **Configure:**
   - **Base URL:** `http://SEU_IP:8001/v1`
   - **API Key:** Qualquer valor (ex: `your-api-key-here`)
   - **Model:** `gemini/gemini-2.0-flash-exp` (ou outro modelo)

5. **Salve e teste!**

### Op√ß√£o 2: Self-Hosting LobeChat (Controle Total)

Se voc√™ quer hospedar o LobeChat voc√™ mesmo:

#### Usando Docker (Recomendado)

```bash
# 1. Clone o reposit√≥rio do LobeChat
git clone https://github.com/lobehub/lobe-chat.git
cd lobe-chat

# 2. Crie arquivo .env
cat > .env << 'EOF'
# API Configuration
OPENAI_API_KEY=your-api-key-here
API_BASE_URL=http://localhost:8001/v1

# Database (opcional)
DATABASE_URL=postgresql://user:password@localhost:5432/lobechat

# Other settings
ACCESS_CODE=your-access-code
EOF

# 3. Inicie com Docker
docker-compose up -d
```

#### Usando Docker Run

```bash
docker run -d \
  --name lobechat \
  -p 3210:3210 \
  -e OPENAI_API_KEY=your-api-key-here \
  -e API_BASE_URL=http://host.docker.internal:8001/v1 \
  -e ACCESS_CODE=your-access-code \
  lobehub/lobe-chat:latest
```

**‚ö†Ô∏è Importante:** Use `host.docker.internal` em vez de `localhost` se o LobeChat estiver no Docker e sua API no host.

#### Configura√ß√£o Avan√ßada

Crie `docker-compose.yml`:

```yaml
version: '3.8'

services:
  lobechat:
    image: lobehub/lobe-chat:latest
    container_name: lobechat
    ports:
      - "3210:3210"
    environment:
      # API Backend
      OPENAI_API_KEY: "your-api-key-here"
      API_BASE_URL: "http://host.docker.internal:8001/v1"
      
      # Access Control
      ACCESS_CODE: "your-access-code"
      
      # Optional: Database
      # DATABASE_URL: "postgresql://user:password@postgres:5432/lobechat"
      
      # Optional: Analytics
      # ENABLE_OAUTH_SSO: "true"
      # NEXTAUTH_URL: "http://localhost:3210"
    restart: unless-stopped
```

Depois execute:

```bash
docker-compose up -d
```

---

## üîß Configura√ß√£o via Interface do LobeChat

### Passo 1: Acessar Configura√ß√µes

1. Abra o LobeChat: `http://localhost:3210`
2. Clique no √≠cone de ‚öôÔ∏è (Configura√ß√µes)
3. V√° para **"Configura√ß√µes do Modelo"** ou **"Model Settings"**

### Passo 2: Adicionar Provider Customizado

1. Clique em **"Adicionar Provider"** ou **"Add Provider"**
2. Selecione **"Custom OpenAI"**
3. Preencha:

```
Nome: Minha API LiteLLM
Base URL: http://localhost:8001/v1
API Key: your-api-key-here
```

### Passo 3: Adicionar Modelos

Adicione os modelos que voc√™ quer usar:

```
gemini/gemini-2.0-flash-exp
openai/gpt-4o
openai/gpt-4o-mini
anthropic/claude-3-opus-20240229
ollama/llama2
```

### Passo 4: Testar

1. Crie uma nova conversa
2. Selecione o modelo que voc√™ configurou
3. Envie uma mensagem
4. ‚úÖ Deve funcionar!

---

## üåê Expor para Internet (Produ√ß√£o)

### Op√ß√£o 1: Ngrok (Desenvolvimento/Teste)

```bash
# Instale o ngrok
curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
sudo apt update && sudo apt install ngrok

# Autentique (crie conta em https://ngrok.com)
ngrok authtoken YOUR_AUTH_TOKEN

# Exponha sua API
ngrok http 8001
```

Copie a URL gerada (ex: `https://abc123.ngrok.io`) e use no LobeChat:

```
Base URL: https://abc123.ngrok.io/v1
```

### Op√ß√£o 2: Cloudflare Tunnel (Produ√ß√£o)

```bash
# Instale o cloudflared
wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
sudo dpkg -i cloudflared-linux-amd64.deb

# Autentique
cloudflared tunnel login

# Crie um tunnel
cloudflared tunnel create my-api

# Configure
cat > ~/.cloudflared/config.yml << 'EOF'
tunnel: my-api
credentials-file: /root/.cloudflared/<tunnel-id>.json

ingress:
  - hostname: api.seudominio.com
    service: http://localhost:8001
  - service: http_status:404
EOF

# Inicie o tunnel
cloudflared tunnel run my-api
```

### Op√ß√£o 3: Nginx Reverse Proxy (Produ√ß√£o)

```nginx
server {
    listen 80;
    server_name api.seudominio.com;

    location / {
        proxy_pass http://localhost:8001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Depois configure HTTPS com Let's Encrypt:

```bash
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d api.seudominio.com
```

---

## üîê Autentica√ß√£o e Seguran√ßa

### Implementar API Key Validation

Edite `/src/api/openai_compatible_routes.py`:

```python
# Adicione no in√≠cio do arquivo
VALID_API_KEYS = {
    "sk-lobechat-xxx": "user1",
    "sk-mobile-app-yyy": "user2",
    # Adicione mais keys aqui
}

def validate_api_key(authorization: Optional[str]) -> bool:
    """Validate API key from Authorization header."""
    if not authorization or not authorization.startswith("Bearer "):
        return False
    
    token = authorization[7:]
    return token in VALID_API_KEYS
```

### Configurar CORS para Produ√ß√£o

Edite `/src/api/main.py`:

```python
# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://lobehub.com",
        "https://seu-lobechat.vercel.app",
        "http://localhost:3210",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## üìä Monitoramento e Logs

### Ver Logs da API

```bash
# Seguir logs em tempo real
tail -f logs/app.log

# Filtrar requisi√ß√µes do LobeChat
tail -f logs/app.log | grep "/v1/"
```

### M√©tricas

A API automaticamente loga:
- ‚úÖ Modelos usados
- ‚úÖ Tokens consumidos
- ‚úÖ Tempo de resposta
- ‚úÖ Erros

---

## üß™ Testar Integra√ß√£o

### Teste 1: Listar Modelos

```bash
curl -X GET http://localhost:8001/v1/models \
  -H "Authorization: Bearer test-key"
```

**‚úÖ Sucesso:** Deve retornar lista de modelos

### Teste 2: Chat Simples

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Authorization: Bearer test-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini/gemini-2.0-flash-exp",
    "messages": [
      {"role": "user", "content": "Ol√°!"}
    ],
    "stream": false
  }'
```

**‚úÖ Sucesso:** Deve retornar resposta do modelo

### Teste 3: Streaming

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Authorization: Bearer test-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini/gemini-2.0-flash-exp",
    "messages": [
      {"role": "user", "content": "Conte uma hist√≥ria curta"}
    ],
    "stream": true
  }'
```

**‚úÖ Sucesso:** Deve ver chunks de texto aparecendo

---

## üêõ Troubleshooting

### Problema: "Invalid or missing API key"

**Solu√ß√£o:** Configure o Bearer token:
- No LobeChat: `your-api-key-here`
- Na requisi√ß√£o: `Authorization: Bearer your-api-key-here`

### Problema: "Model not supported"

**Solu√ß√£o:** Use formato `provider/model-name`:
- ‚úÖ Correto: `gemini/gemini-2.0-flash-exp`
- ‚ùå Errado: `gemini-2.0-flash-exp`

### Problema: CORS Error

**Solu√ß√£o:** Adicione a origem do LobeChat ao CORS:

```python
allow_origins=[
    "http://localhost:3210",  # LobeChat local
    "https://lobehub.com",    # LobeChat hospedado
]
```

### Problema: Connection Refused

**Solu√ß√µes:**
1. Verifique se a API est√° rodando: `curl http://localhost:8001/health`
2. Se LobeChat est√° no Docker, use `host.docker.internal` em vez de `localhost`
3. Verifique firewall: `sudo ufw allow 8001`

### Problema: SSL Error (LobeChat hospedado ‚Üí sua API local)

**Solu√ß√£o:** Use ngrok ou cloudflare tunnel para expor com HTTPS

---

## üéâ Conclus√£o

Agora voc√™ tem:

‚úÖ **API FastAPI** expondo endpoints compat√≠veis com OpenAI  
‚úÖ **LiteLLM** como proxy unificado  
‚úÖ **LobeChat** como frontend  
‚úÖ **100+ modelos** dispon√≠veis  

**Acesse:** `http://localhost:3210` (LobeChat)  
**API:** `http://localhost:8001/v1` (Sua API)  
**Docs:** `http://localhost:8001/docs` (Swagger)

---

## üìö Recursos Adicionais

- [LobeChat Documentation](https://lobehub.com/docs)
- [LiteLLM Documentation](https://docs.litellm.ai/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)

---

**√öltima atualiza√ß√£o:** 2025-11-12

