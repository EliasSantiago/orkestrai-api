# üîÑ Model Override - Guia Completo

## üìã Vis√£o Geral

Agora voc√™ pode **sobrescrever o modelo LLM** do agente diretamente no payload da requisi√ß√£o. Isso permite:

‚úÖ Trocar de modelo quando um estiver sobrecarregado (erro 503)  
‚úÖ Testar diferentes modelos com o mesmo agente  
‚úÖ Usar modelos mais r√°pidos/baratos para queries simples  
‚úÖ Flexibilidade total sem precisar modificar o agente  

## üöÄ Como Usar

### Sintaxe B√°sica

Adicione o campo `model` no payload da requisi√ß√£o:

```bash
curl -X 'POST' \
  'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Sua mensagem aqui",
    "session_id": "sua-session-id",
    "model": "gpt-4o-mini"
  }'
```

### Campos do Payload

| Campo | Tipo | Obrigat√≥rio | Descri√ß√£o |
|-------|------|-------------|-----------|
| `agent_id` | integer | ‚úÖ Sim | ID do agente |
| `message` | string | ‚úÖ Sim | Mensagem para o agente |
| `session_id` | string | ‚ùå N√£o | ID da sess√£o (auto-gerado se n√£o fornecido) |
| `model` | string | ‚ùå N√£o | Modelo LLM para usar (sobrescreve o padr√£o do agente) |

## üìä Modelos Dispon√≠veis

### Listar Todos os Modelos

Para ver todos os modelos dispon√≠veis na sua instala√ß√£o:

```bash
curl -X 'POST' \
  'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 1,
    "message": "teste",
    "model": "modelo-invalido"
  }'
```

O erro retornar√° a lista de modelos dispon√≠veis.

### Modelos Comuns

#### **Google Gemini** (via ADK)
- `gemini-2.0-flash-exp` (padr√£o)
- `gemini-2.5-flash`
- `gemini-2.0-flash-thinking-exp`
- `gemini-1.5-pro-latest`
- `gemini-1.5-flash-latest`

#### **OpenAI** (se configurado)
- `gpt-4o`
- `gpt-4o-mini` ‚≠ê (r√°pido e barato)
- `gpt-4-turbo`
- `gpt-3.5-turbo`

#### **Claude** (se configurado via OpenAI-compatible endpoint)
- `claude-3-5-sonnet-latest`
- `claude-3-opus-latest`
- `claude-3-haiku-latest`

#### **Ollama** (se configurado)
- Qualquer modelo instalado no Ollama local
- Ex: `llama3.1`, `mistral`, `phi3`, etc.

#### **On-Premise** (se configurado)
- Modelos configurados no servidor on-premise

## üí° Exemplos Pr√°ticos

### Exemplo 1: Trocar Modelo por Sobrecarga (503)

**Cen√°rio:** Seu agente usa `gemini-2.5-flash`, mas est√° recebendo erro 503.

```bash
# ‚ùå Usando modelo padr√£o (sobrecarregado)
curl -X 'POST' \
  'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Pesquise sobre...",
    "session_id": "abc123"
  }'

# Resposta:
# {"detail": "503 UNAVAILABLE. The model is overloaded."}

# ‚úÖ Trocando para outro modelo
curl -X 'POST' \
  'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Pesquise sobre...",
    "session_id": "abc123",
    "model": "gpt-4o-mini"
  }'

# Resposta:
# {
#   "response": "Resposta do agente...",
#   "agent_id": 5,
#   "agent_name": "Assistente de Pesquisa",
#   "session_id": "abc123",
#   "model_used": "gpt-4o-mini"
# }
```

### Exemplo 2: Seu Caso Espec√≠fico

```bash
curl -X 'POST' \
  'http://localhost:8001/api/agents/chat' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwidXNlcl9pZCI6MiwiZXhwIjoxNzY1MzgxODIyfQ.3Fa34NlIGldX7m3TKKN2fveptCgkXkmmswV-2Mdyk00' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Quero a lista de resultados dos jogos de ontem pelo campeonato brasileiro",
    "session_id": "f88381f9-a28f-4029-886c-15425ec4745a",
    "model": "gpt-4o-mini"
  }'
```

### Exemplo 3: Testar Diferentes Modelos

```bash
# Teste com Gemini (r√°pido, gr√°tis)
curl -X 'POST' 'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Teste de modelo",
    "model": "gemini-2.5-flash"
  }'

# Teste com GPT-4o-mini (r√°pido, barato)
curl -X 'POST' 'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Teste de modelo",
    "model": "gpt-4o-mini"
  }'

# Teste com GPT-4o (mais inteligente, mais caro)
curl -X 'POST' 'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Teste de modelo",
    "model": "gpt-4o"
  }'
```

### Exemplo 4: Usar Modelo Local (Ollama)

```bash
# Use modelo local instalado no Ollama
curl -X 'POST' 'http://localhost:8001/api/agents/chat' \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "agent_id": 5,
    "message": "Pergunta simples",
    "model": "llama3.1"
  }'
```

## üéØ Resposta com model_used

A resposta agora inclui qual modelo foi usado:

```json
{
  "response": "Aqui est√° a lista dos resultados...",
  "agent_id": 5,
  "agent_name": "Assistente de Pesquisa Avan√ßada",
  "session_id": "f88381f9-a28f-4029-886c-15425ec4745a",
  "model_used": "gpt-4o-mini"
}
```

## ‚ö†Ô∏è Notas Importantes

### 1. Valida√ß√£o de Modelo

Se voc√™ especificar um modelo inv√°lido, receber√° um erro 400:

```json
{
  "detail": "Model 'modelo-invalido' is not supported. Available models: {...}"
}
```

### 2. Compatibilidade com Ferramentas

Todos os modelos suportam as mesmas ferramentas (tools). O model override **n√£o afeta** as ferramentas dispon√≠veis para o agente.

### 3. Continuidade de Sess√£o

Voc√™ pode trocar de modelo **durante a mesma sess√£o**:

```bash
# Mensagem 1 - usa gemini
curl -X 'POST' ... -d '{
  "agent_id": 5,
  "message": "Primeira pergunta",
  "session_id": "abc123"
}'

# Mensagem 2 - troca para gpt-4o-mini
curl -X 'POST' ... -d '{
  "agent_id": 5,
  "message": "Segunda pergunta",
  "session_id": "abc123",
  "model": "gpt-4o-mini"
}'
```

A conversa continua, mesmo trocando de modelo!

### 4. Performance vs Custo

| Modelo | Velocidade | Custo | Intelig√™ncia | Uso Recomendado |
|--------|-----------|-------|--------------|-----------------|
| `gpt-4o-mini` | ‚ö°‚ö°‚ö° | üí∞ | ‚≠ê‚≠ê‚≠ê | Queries simples, alta volume |
| `gemini-2.5-flash` | ‚ö°‚ö°‚ö° | üí∞ Gr√°tis | ‚≠ê‚≠ê‚≠ê | Uso geral, r√°pido |
| `gpt-4o` | ‚ö°‚ö° | üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Tarefas complexas |
| `claude-3-5-sonnet` | ‚ö°‚ö° | üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | An√°lise profunda |

## üîß Troubleshooting

### Erro: "Model not supported"

**Solu√ß√£o:** Verifique se o modelo est√° dispon√≠vel:
1. Veja a lista de modelos suportados no erro
2. Verifique se as API keys est√£o configuradas no `.env`
3. Para Ollama, verifique se o modelo est√° instalado

### Erro: "503 UNAVAILABLE"

**Solu√ß√£o:** Troque para outro modelo:
- De `gemini-*` para `gpt-4o-mini`
- De `gpt-*` para `gemini-2.5-flash`
- Use modelo local (Ollama) se dispon√≠vel

### Modelo n√£o aparece na lista

**Causa:** API key n√£o configurada no `.env`

**Solu√ß√£o:**
- OpenAI: Configure `OPENAI_API_KEY`
- Gemini: Configure `GOOGLE_API_KEY`
- Ollama: Configure `OLLAMA_API_BASE_URL`
- On-premise: Configure `ONPREMISE_API_BASE_URL`

## üìö Documenta√ß√£o da API

### Swagger/OpenAPI

Acesse a documenta√ß√£o interativa:
```
http://localhost:8001/docs
```

L√° voc√™ pode:
- Ver todos os modelos dispon√≠veis
- Testar o endpoint `/api/agents/chat`
- Ver exemplos de requisi√ß√µes

## ‚úÖ Checklist

- [ ] Entendi como adicionar `model` no payload
- [ ] Sei quais modelos est√£o dispon√≠veis na minha instala√ß√£o
- [ ] Testei trocar de modelo quando receber erro 503
- [ ] Verifico o `model_used` na resposta
- [ ] Sei como listar modelos dispon√≠veis

---

**Data de implementa√ß√£o:** 10 de novembro de 2025  
**Status:** ‚úÖ Funcionalidade implementada e testada  
**Compatibilidade:** Todos os agentes existentes continuam funcionando normalmente

