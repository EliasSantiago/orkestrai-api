# ConfiguraÃ§Ã£o de MÃºltiplos Provedores LLM

Este documento explica como configurar e usar mÃºltiplos provedores LLM na aplicaÃ§Ã£o.

## ğŸ“‹ Provedores Suportados

A aplicaÃ§Ã£o agora suporta trÃªs tipos de provedores LLM:

1. **Google ADK (Gemini)** - Modelos Gemini via Google ADK
2. **OpenAI** - Modelos GPT via OpenAI API
3. **On-Premise** - Modelos LLM locais via API compatÃ­vel com OpenAI

## ğŸ”§ ConfiguraÃ§Ã£o

### 1. Google Gemini (ADK)

Adicione no `.env`:
```env
GOOGLE_API_KEY=sua_chave_gemini_aqui
```

Modelos suportados:
- `gemini-2.0-flash-exp`
- `gemini-2.0-flash-thinking-exp`
- `gemini-1.5-pro`
- `gemini-1.5-flash`
- `gemini-1.5-flash-8b`
- E outros modelos Gemini

### 2. OpenAI

Adicione no `.env`:
```env
OPENAI_API_KEY=sua_chave_openai_aqui
```

Modelos suportados:
- `gpt-4o`
- `gpt-4o-mini`
- `gpt-4-turbo`
- `gpt-4`
- `gpt-3.5-turbo`
- `o1-preview`
- `o1-mini`

### 3. On-Premise (Modelos Locais)

Adicione no `.env`:
```env
# URL base da API (deve ser compatÃ­vel com OpenAI API)
ONPREMISE_API_BASE_URL=http://localhost:1234

# Chave de API (opcional, algumas APIs locais nÃ£o precisam)
ONPREMISE_API_KEY=opcional

# Lista de modelos disponÃ­veis (separados por vÃ­rgula)
ONPREMISE_MODELS=llama-2-7b,mixtral-8x7b,llama-3-70b
```

**Nota:** A API on-premise deve ser compatÃ­vel com a especificaÃ§Ã£o OpenAI API (endpoint `/v1/chat/completions`).

## ğŸš€ Uso

### Criar um Agente com Modelo EspecÃ­fico

Ao criar um agente, vocÃª pode especificar qualquer modelo suportado:

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Agente Gemini",
    "model": "gemini-2.0-flash-exp",
    "description": "Agente usando Gemini",
    "instruction": "VocÃª Ã© um assistente Ãºtil."
  }'
```

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Agente OpenAI",
    "model": "gpt-4o-mini",
    "description": "Agente usando OpenAI",
    "instruction": "VocÃª Ã© um assistente Ãºtil."
  }'
```

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Agente Local",
    "model": "llama-2-7b",
    "description": "Agente usando modelo local",
    "instruction": "VocÃª Ã© um assistente Ãºtil."
  }'
```

### Listar Modelos Suportados

```bash
curl http://localhost:8001/api/models
```

Resposta:
```json
{
  "providers": {
    "ADK": [
      "gemini-2.0-flash-exp",
      "gemini-1.5-pro",
      ...
    ],
    "OpenAI": [
      "gpt-4o",
      "gpt-4o-mini",
      ...
    ],
    "OnPremise": [
      "llama-2-7b",
      "mixtral-8x7b",
      ...
    ]
  }
}
```

### Chat com Agente

O chat funciona automaticamente com qualquer modelo:

```bash
curl -X POST http://localhost:8001/api/agents/chat \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "OlÃ¡, como vocÃª pode me ajudar?",
    "agent_id": 1
  }'
```

A aplicaÃ§Ã£o automaticamente:
1. Identifica o modelo do agente
2. Seleciona o provider apropriado
3. Executa a requisiÃ§Ã£o usando o provider correto

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Routes     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Factory        â”‚  â† Escolhe o provider
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  ADK  â”‚ â”‚OpenAIâ”‚    â”‚OnPremiseâ”‚
â”‚(Gemini)â”‚ â”‚      â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Detalhes TÃ©cnicos

### Como Funciona

1. **Factory Pattern**: A `LLMFactory` verifica qual provider suporta o modelo solicitado
2. **Provider Abstraction**: Cada provider implementa a interface `LLMProvider`
3. **Automatic Selection**: A aplicaÃ§Ã£o escolhe automaticamente o provider baseado no nome do modelo

### Adicionar Novo Provedor

Para adicionar um novo provedor (ex: Anthropic Claude):

1. Crie `src/core/llm_providers/anthropic_provider.py`
2. Implemente a classe herdando de `LLMProvider`
3. Adicione no `LLMFactory._get_providers()`
4. Configure as variÃ¡veis de ambiente necessÃ¡rias

## âš ï¸ Notas Importantes

1. **Contexto de Conversa**: 
   - Modelos Gemini (ADK) tÃªm suporte completo para contexto via Redis
   - Outros provedores tambÃ©m suportam contexto, mas podem ter limitaÃ§Ãµes

2. **Ferramentas (Tools)**:
   - Modelos Gemini (ADK) tÃªm suporte completo para tools
   - OpenAI suporta function calling
   - On-premise depende da implementaÃ§Ã£o da API

3. **Rate Limiting**:
   - Todos os providers tÃªm retry automÃ¡tico para erros 429
   - Backoff exponencial: 2s, 4s, 8s

## ğŸ§ª Testando

1. Configure pelo menos um provedor no `.env`
2. Inicie a aplicaÃ§Ã£o: `./scripts/start_backend.sh`
3. Liste modelos: `curl http://localhost:8001/api/models`
4. Crie um agente com o modelo desejado
5. Teste o chat

## ğŸ“š ReferÃªncias

- [Google ADK Documentation](https://github.com/google/adk)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [OpenAI-Compatible API Spec](https://platform.openai.com/docs/api-reference)

