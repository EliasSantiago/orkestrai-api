# Configura√ß√£o do Provedor Ollama

Este guia explica como configurar e usar o provedor Ollama para modelos LLM locais.

## üìã Vis√£o Geral

O Ollama √© uma ferramenta para executar modelos LLM localmente. Este provedor permite usar modelos Ollama diretamente na aplica√ß√£o.

## üîß Configura√ß√£o

### 1. Adicionar Vari√°veis no `.env`

Adicione as seguintes vari√°veis de ambiente:

```env
# URL base da API Ollama (geralmente localhost:11434)
OLLAMA_API_BASE_URL=http://localhost:11434

# Lista de modelos dispon√≠veis (OPCIONAL - separados por v√≠rgula)
# Se n√£o configurar, qualquer modelo Ollama ser√° aceito
# OLLAMA_MODELS=gemma-2b-light:latest,llama2:7b,mistral:latest
```

### 2. Configura√ß√£o M√≠nima

```env
# Configura√ß√£o m√≠nima
OLLAMA_API_BASE_URL=http://localhost:11434
```

### 3. Configura√ß√£o com Lista de Modelos

```env
# Configura√ß√£o com lista de modelos (valida√ß√£o antecipada)
OLLAMA_API_BASE_URL=http://localhost:11434
OLLAMA_MODELS=gemma-2b-light:latest,llama2:7b,mistral:latest
```

## üöÄ Como Funciona

### Endpoint Usado

O provedor Ollama usa o endpoint:
```
{OLLAMA_API_BASE_URL}/api/generate
```

Exemplo:
```
http://localhost:11434/api/generate
```

### Formato da Requisi√ß√£o

O provedor converte automaticamente as mensagens para o formato Ollama:

**Entrada (formato interno):**
```json
{
  "messages": [
    {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
    {"role": "user", "content": "Ol√°"}
  ],
  "model": "gemma-2b-light:latest"
}
```

**Enviado para Ollama:**
```json
{
  "model": "gemma-2b-light:latest",
  "prompt": "System: Voc√™ √© um assistente √∫til.\n\nUser: Ol√°",
  "stream": true,
  "options": {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.1,
    "num_ctx": 1024
  }
}
```

### Formato da Resposta

Ollama retorna JSON lines (uma linha JSON por chunk):
```json
{"response": "Ol√°! Como posso ajudar?", "done": false}
{"response": " Estou aqui para", "done": false}
{"response": " responder suas perguntas.", "done": true}
```

O provedor extrai automaticamente o campo `response` e faz stream dos chunks.

## üìù Exemplo de Uso

### 1. Criar Agente com Modelo Ollama

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente Ollama",
    "model": "gemma-2b-light:latest",
    "description": "Assistente usando modelo Ollama local",
    "instruction": "Voc√™ √© um assistente √∫til e prestativo."
  }'
```

### 2. Testar Chat

```bash
curl -X POST http://localhost:8001/api/agents/chat \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "agent_id": 1,
    "message": "Ol√°, como voc√™ pode me ajudar?",
    "session_id": ""
  }'
```

## üîç Detec√ß√£o de Modelos Ollama

O provedor detecta automaticamente modelos Ollama pelos seguintes padr√µes:

- ‚úÖ Modelos com `:` (ex: `gemma-2b-light:latest`, `llama2:7b`)
- ‚úÖ Modelos que come√ßam com prefixos comuns:
  - `llama` (ex: `llama2`, `llama3`)
  - `mistral` (ex: `mistral:latest`)
  - `gemma` (ex: `gemma-2b-light:latest`)
  - `phi` (ex: `phi-2`)
  - `codellama` (ex: `codellama:7b`)
  - `neural-chat` (ex: `neural-chat:latest`)
  - `starling` (ex: `starling-lm:latest`)

### Exemplos de Modelos Suportados

| Modelo | Detectado? | Motivo |
|--------|------------|--------|
| `gemma-2b-light:latest` | ‚úÖ Sim | Tem `:` |
| `llama2:7b` | ‚úÖ Sim | Tem `:` |
| `mistral:latest` | ‚úÖ Sim | Tem `:` e prefixo `mistral` |
| `llama3` | ‚úÖ Sim | Prefixo `llama` |
| `gpt-4o` | ‚ùå N√£o | N√£o √© modelo Ollama |

## ‚öôÔ∏è Par√¢metros Opcionais

Voc√™ pode passar par√¢metros adicionais no chat:

```python
# Exemplo de uso program√°tico
provider = OllamaProvider()
async for chunk in provider.chat(
    messages=[...],
    model="gemma-2b-light:latest",
    temperature=0.8,  # Personalizado
    top_p=0.95,       # Personalizado
    top_k=50,         # Personalizado
    repeat_penalty=1.2, # Personalizado
    num_ctx=2048      # Personalizado
):
    print(chunk, end="")
```

## üîß Troubleshooting

### Erro: "OLLAMA_API_BASE_URL not configured"

**Solu√ß√£o:** Adicione `OLLAMA_API_BASE_URL` no `.env`:
```env
OLLAMA_API_BASE_URL=http://localhost:11434
```

### Erro: "Connection refused"

**Causa:** Servidor Ollama n√£o est√° rodando.

**Solu√ß√£o:**
1. Verifique se o Ollama est√° instalado e rodando:
   ```bash
   curl http://localhost:11434/api/tags
   ```
2. Se n√£o estiver rodando, inicie o Ollama:
   ```bash
   ollama serve
   ```

### Erro: "Model not found"

**Causa:** Modelo n√£o est√° dispon√≠vel no Ollama.

**Solu√ß√£o:**
1. Liste modelos dispon√≠veis:
   ```bash
   curl http://localhost:11434/api/tags
   ```
2. Baixe o modelo se necess√°rio:
   ```bash
   ollama pull gemma-2b-light:latest
   ```

### Modelo sendo roteado para outro provider

**Causa:** Modelo n√£o est√° sendo detectado como Ollama.

**Solu√ß√£o:**
1. Use formato com `:` (ex: `gemma-2b-light:latest`)
2. Ou configure `OLLAMA_MODELS` com o nome exato:
   ```env
   OLLAMA_MODELS=gemma-2b-light:latest
   ```

## üìö Refer√™ncias

- [Ollama Documentation](https://ollama.ai/docs)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Available Models](https://ollama.ai/library)

## ‚úÖ Checklist

- [ ] Ollama instalado e rodando
- [ ] `OLLAMA_API_BASE_URL` configurado no `.env`
- [ ] Modelo baixado no Ollama (ex: `ollama pull gemma-2b-light:latest`)
- [ ] Aplica√ß√£o reiniciada
- [ ] Agente criado com modelo Ollama
- [ ] Chat testado com sucesso

