# Modelos On-Premise Dispon√≠veis

Lista atualizada dos modelos dispon√≠veis na API on-premise em **11/11/2025**.

## üì¶ Modelos Dispon√≠veis

### Modelos de C√≥digo (Code Models)

| Modelo | Tamanho | Descri√ß√£o |
|--------|---------|-----------|
| `qwen3-coder:30b` | 30B | Qwen3 Coder - Especializado em programa√ß√£o |

### Modelos de Racioc√≠nio (Reasoning Models)

| Modelo | Tamanho | Descri√ß√£o |
|--------|---------|-----------|
| `deepseek-r1:14b` | 14B | DeepSeek R1 - Racioc√≠nio avan√ßado |
| `deepseek-r1:8b` | 8B | DeepSeek R1 - Vers√£o menor |
| `deepseek-r1:1.5b-qwen-distill-fp16` | 1.5B | DeepSeek R1 distilado (FP16) |

### Modelos Qwen (Qwen Family)

| Modelo | Tamanho | Quantiza√ß√£o | Descri√ß√£o |
|--------|---------|-------------|-----------|
| `qwen3:30b-a3b-instruct-2507-q4_K_M` | 30B | Q4_K_M | Qwen3 Instruct quantizado |
| `qwen3:30b-a3b-instruct-2507-fp16` | 30B | FP16 | Qwen3 Instruct precis√£o total |
| `qwen3:14b` | 14B | - | Qwen3 base |
| `qwen2.5:7b-instruct-fp16` | 7B | FP16 | Qwen 2.5 Instruct |
| `qwen2.5:14b` | 14B | - | Qwen 2.5 base |

### Modelos Gemma (Google Gemma)

| Modelo | Tamanho | Quantiza√ß√£o | Descri√ß√£o |
|--------|---------|-------------|-----------|
| `gemma3:27b-it-q4_K_M` | 27B | Q4_K_M | Gemma3 Instruct quantizado |
| `gemma3:12b-it-fp16` | 12B | FP16 | Gemma3 Instruct precis√£o total |
| `gemma3:12b-it-q4_K_M` | 12B | Q4_K_M | Gemma3 Instruct quantizado |
| `gemma3:12b` | 12B | - | Gemma3 base |
| `gemma3:latest` | - | - | Gemma3 vers√£o mais recente |

### Modelos Llama (Meta Llama)

| Modelo | Tamanho | Quantiza√ß√£o | Descri√ß√£o |
|--------|---------|-------------|-----------|
| `llama3.1:8b-instruct-fp16` | 8B | FP16 | Llama 3.1 Instruct precis√£o total |
| `llama3.1:8b` | 8B | - | Llama 3.1 base |
| `llama3.2:3b` | 3B | - | Llama 3.2 compacto |

### Outros Modelos

| Modelo | Tamanho | Descri√ß√£o |
|--------|---------|-----------|
| `gpt-oss:20b` | 20B | GPT Open Source |
| `phi4:14b` | 14B | Microsoft Phi-4 |
| `nomic-embed-text:latest` | - | Modelo de embeddings |

## üéØ Como Usar os Modelos

### 1. Modelo Recomendado para Uso Geral

**`qwen3:30b-a3b-instruct-2507-q4_K_M`** - Bom equil√≠brio entre qualidade e velocidade

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente Geral",
    "description": "Assistente vers√°til para diversas tarefas",
    "model": "qwen3:30b-a3b-instruct-2507-q4_K_M",
    "instruction": "Voc√™ √© um assistente √∫til que responde em portugu√™s brasileiro.",
    "tools": ["calculator", "get_current_time"]
  }'
```

### 2. Modelo para Programa√ß√£o

**`qwen3-coder:30b`** - Especializado em c√≥digo

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente de Programa√ß√£o",
    "description": "Especialista em c√≥digo e desenvolvimento",
    "model": "qwen3-coder:30b",
    "instruction": "Voc√™ √© um expert em programa√ß√£o. Ajude com c√≥digo, debugging e boas pr√°ticas.",
    "tools": []
  }'
```

### 3. Modelo R√°pido e Leve

**`llama3.2:3b`** - Menor e mais r√°pido

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente R√°pido",
    "description": "Respostas r√°pidas para tarefas simples",
    "model": "llama3.2:3b",
    "instruction": "Voc√™ √© um assistente r√°pido e direto. Seja conciso nas respostas.",
    "tools": []
  }'
```

### 4. Modelo para Racioc√≠nio Complexo

**`deepseek-r1:14b`** - Melhor para problemas complexos

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente de Racioc√≠nio",
    "description": "Especialista em problemas complexos e an√°lise profunda",
    "model": "deepseek-r1:14b",
    "instruction": "Voc√™ √© um especialista em resolver problemas complexos. Analise profundamente antes de responder.",
    "tools": ["calculator"]
  }'
```

### 5. Modelo Premium (Alta Qualidade)

**`gemma3:27b-it-q4_K_M`** - Melhor qualidade

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente Premium",
    "description": "Respostas de alta qualidade para tarefas importantes",
    "model": "gemma3:27b-it-q4_K_M",
    "instruction": "Voc√™ √© um assistente de elite. Forne√ßa respostas detalhadas e precisas.",
    "tools": ["calculator", "get_current_time"]
  }'
```

## üìä Compara√ß√£o de Modelos

### Por Tamanho

| Categoria | Modelos | Uso Recomendado |
|-----------|---------|-----------------|
| **Pequeno (1.5-3B)** | `llama3.2:3b`, `deepseek-r1:1.5b-qwen-distill-fp16` | Respostas r√°pidas, tarefas simples |
| **M√©dio (7-8B)** | `qwen2.5:7b-instruct-fp16`, `llama3.1:8b`, `deepseek-r1:8b` | Uso geral, bom equil√≠brio |
| **Grande (12-14B)** | `gemma3:12b`, `qwen3:14b`, `phi4:14b`, `deepseek-r1:14b` | Tarefas complexas, alta qualidade |
| **Extra Grande (20-30B)** | `gpt-oss:20b`, `qwen3:30b`, `gemma3:27b`, `qwen3-coder:30b` | M√°xima qualidade, tarefas cr√≠ticas |

### Por Velocidade vs Qualidade

```
R√°pido          ‚Üê‚Üí          Qualidade
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
llama3.2:3b
‚îú‚îÄ qwen2.5:7b-instruct-fp16
‚îú‚îÄ llama3.1:8b
‚îú‚îÄ gemma3:12b-it-q4_K_M (quantizado)
‚îú‚îÄ qwen3:14b
‚îú‚îÄ deepseek-r1:14b
‚îú‚îÄ gpt-oss:20b
‚îú‚îÄ gemma3:27b-it-q4_K_M
‚îî‚îÄ qwen3:30b-a3b-instruct-2507-fp16 (FP16 m√°xima qualidade)
```

### Por Especializa√ß√£o

| Especializa√ß√£o | Modelos Recomendados |
|----------------|---------------------|
| **Programa√ß√£o** | `qwen3-coder:30b` |
| **Racioc√≠nio** | `deepseek-r1:14b`, `deepseek-r1:8b` |
| **Uso Geral** | `qwen3:30b-a3b-instruct-2507-q4_K_M`, `gemma3:12b-it-fp16`, `llama3.1:8b-instruct-fp16` |
| **Multil√≠ngue** | `qwen3:30b`, `qwen2.5:7b` |
| **R√°pido/Leve** | `llama3.2:3b`, `deepseek-r1:1.5b-qwen-distill-fp16` |

## üîç Entendendo Quantiza√ß√£o

### Tipos de Quantiza√ß√£o

- **FP16** (Float16): Precis√£o m√°xima, maior tamanho e uso de mem√≥ria
- **Q4_K_M** (4-bit): Boa qualidade, menor tamanho (~75% menor que FP16)
- **Q4_K_S** (4-bit Small): Ainda menor, ligeira perda de qualidade

### Quando Usar Cada Tipo

| Tipo | Velocidade | Qualidade | Mem√≥ria | Uso Recomendado |
|------|-----------|-----------|---------|-----------------|
| **FP16** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Alta | Tarefas cr√≠ticas |
| **Q4_K_M** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | M√©dia | Uso geral (recomendado) |
| **Base** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Vari√°vel | Depende do modelo |

## ‚ö†Ô∏è Importante: Conflitos de Nomes

### ‚úÖ Sem Conflito

Todos os modelos on-premise t√™m `:` no nome, o que evita conflitos:

- `gemma3:12b` ‚Üí **On-Premise** ‚úÖ
- `gemini-2.0-flash` ‚Üí **Google Gemini API** ‚úÖ
- `llama3.1:8b` ‚Üí **On-Premise** ‚úÖ
- `gpt-oss:20b` ‚Üí **On-Premise** ‚úÖ
- `gpt-4o` ‚Üí **OpenAI API** ‚úÖ

### Regra de Detec√ß√£o

O sistema detecta automaticamente:

1. **Com `:` (dois-pontos)** ‚Üí On-Premise
2. **Prefixo `gemini-`** ‚Üí Google Gemini
3. **Prefixo `gpt-4`, `gpt-3.5`** (sem `:`) ‚Üí OpenAI
4. **Prefixo `local-` ou `onpremise-`** ‚Üí On-Premise

## üöÄ Script: Criar Agentes para Todos os Modelos

```python
#!/usr/bin/env python3
"""Cria agentes para cada modelo dispon√≠vel na API on-premise."""

import requests

API_URL = "http://localhost:8001"
TOKEN = "SEU_TOKEN_AQUI"

MODELS = {
    "Programa√ß√£o": "qwen3-coder:30b",
    "Racioc√≠nio": "deepseek-r1:14b",
    "Geral Premium": "qwen3:30b-a3b-instruct-2507-q4_K_M",
    "R√°pido": "llama3.2:3b",
    "Alta Qualidade": "gemma3:27b-it-q4_K_M",
}

for name, model in MODELS.items():
    response = requests.post(
        f"{API_URL}/api/agents",
        headers={
            "Authorization": f"Bearer {TOKEN}",
            "Content-Type": "application/json"
        },
        json={
            "name": f"Assistente {name}",
            "description": f"Agente usando {model}",
            "model": model,
            "instruction": "Voc√™ √© um assistente √∫til em portugu√™s brasileiro.",
            "tools": ["calculator", "get_current_time"]
        }
    )
    
    if response.status_code == 201:
        agent = response.json()
        print(f"‚úÖ Criado: {agent['name']} (ID: {agent['id']})")
    else:
        print(f"‚ùå Erro ao criar {name}: {response.text}")
```

## üìö Recursos Adicionais

- **Endpoint da API**: `https://apidesenv.go.gov.br/ia/modelos-linguagem-natural/v2.0/`
- **Endpoint de Chat**: `/chat`
- **Endpoint de Modelos**: `/models`
- **Autentica√ß√£o**: OAuth 2.0 (client_credentials)

## üÜò Suporte

Se um modelo n√£o estiver funcionando:

1. Verifique se est√° na lista de modelos dispon√≠veis
2. Confirme que o nome est√° escrito corretamente (case-sensitive)
3. Use o formato exato com `:` (ex: `qwen3:30b`, n√£o `qwen3-30b`)
4. Verifique os logs da aplica√ß√£o para detalhes do erro

---

**√öltima atualiza√ß√£o**: 11/11/2025

