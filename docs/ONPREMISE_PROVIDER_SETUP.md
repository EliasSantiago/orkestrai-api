# Configura√ß√£o do Provedor On-Premise

Este guia explica como configurar e usar o provedor LLM on-premise na aplica√ß√£o.

## üìã Vis√£o Geral

O provedor on-premise permite que voc√™ use modelos de linguagem hospedados em infraestrutura pr√≥pria, com suporte a:
- Autentica√ß√£o OAuth 2.0 (client_credentials ou password grant)
- Formato de API similar ao Ollama/OpenAI
- Streaming de respostas em tempo real
- Configura√ß√£o flex√≠vel de par√¢metros

## üîê Autentica√ß√£o OAuth

O provedor on-premise usa autentica√ß√£o OAuth 2.0 para gerar tokens de acesso automaticamente.

### Vari√°veis de Ambiente Necess√°rias

Adicione as seguintes vari√°veis no arquivo `.env`:

```env
# URL base da API on-premise
ONPREMISE_API_BASE_URL=https://apidesenv.go.gov.br/ia/modelos-linguagem-natural/v2.0/

# Endpoint do chat (relativo √† URL base)
ONPREMISE_CHAT_ENDPOINT=/api/chat

# URL para gerar token OAuth
ONPREMISE_TOKEN_URL=https://apidesenv.go.gov.br/token

# Consumer Key e Secret (credenciais OAuth)
ONPREMISE_CONSUMER_KEY=sua_consumer_key_aqui
ONPREMISE_CONSUMER_SECRET=sua_consumer_secret_aqui

# Tipo de grant OAuth (client_credentials ou password)
ONPREMISE_OAUTH_GRANT_TYPE=client_credentials

# Para grant type "password", adicione:
# ONPREMISE_USERNAME=seu_usuario
# ONPREMISE_PASSWORD=sua_senha

# Verifica√ß√£o SSL (false para certificados autoassinados)
VERIFY_SSL=false

# Opcional: Lista de modelos suportados (separados por v√≠rgula)
# Se n√£o especificado, qualquer nome de modelo ser√° aceito
ONPREMISE_MODELS=modelo1,modelo2,modelo3
```

## üì¶ Formato do Payload

O provedor on-premise envia requisi√ß√µes no seguinte formato:

```json
{
  "model": "nome-do-modelo",
  "messages": [
    {
      "role": "system",
      "content": "Voc√™ √© um assistente √∫til."
    },
    {
      "role": "user",
      "content": "Ol√°, como voc√™ est√°?"
    }
  ],
  "stream": true,
  "options": {
    "temperature": 0.1,
    "top_p": 0.15,
    "top_k": 0,
    "num_predict": 500,
    "repeat_penalty": 1.1,
    "num_ctx": 4096,
    "seed": 0
  },
  "format": "string",
  "keep_alive": "5m"
}
```

### Campos Principais

- **model**: Nome do modelo a ser usado
- **messages**: Array de mensagens da conversa (role: system/user/assistant)
- **stream**: true para respostas em streaming, false para resposta completa
- **options**: Par√¢metros de gera√ß√£o do modelo
  - **temperature**: Controle de aleatoriedade (0.0 a 1.0, padr√£o: 0.1)
  - **top_p**: Nucleus sampling (0.0 a 1.0, padr√£o: 0.15)
  - **top_k**: Top-K sampling (padr√£o: 0 = desabilitado)
  - **num_predict**: N√∫mero m√°ximo de tokens a gerar (padr√£o: 500)
  - **repeat_penalty**: Penalidade para repeti√ß√£o (padr√£o: 1.1)
  - **num_ctx**: Tamanho da janela de contexto (padr√£o: 4096)
  - **seed**: Semente para reprodutibilidade (opcional)
- **format**: Formato da resposta (opcional, ex: "json")
- **keep_alive**: Tempo para manter o modelo carregado (opcional, ex: "5m")

## üîÑ Formatos de Resposta Suportados

O provedor on-premise suporta m√∫ltiplos formatos de resposta:

### 1. Formato OpenAI (SSE)
```json
data: {"choices": [{"delta": {"content": "texto"}}]}
data: [DONE]
```

### 2. Formato Ollama /api/chat
```json
{"message": {"role": "assistant", "content": "texto"}, "done": false}
{"message": {"role": "assistant", "content": ""}, "done": true}
```

### 3. Formato Ollama /api/generate
```json
{"response": "texto", "done": false}
{"response": "", "done": true}
```

### 4. Formato Direto
```json
{"content": "texto"}
```

## üöÄ Como Usar

### 1. Via API REST

Crie um agente com o modelo on-premise:

```bash
curl -X POST http://localhost:8001/api/agents \
  -H "Authorization: Bearer seu_token" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistente On-Premise",
    "description": "Assistente usando modelo on-premise",
    "model": "nome-do-modelo-onpremise",
    "instruction": "Voc√™ √© um assistente √∫til."
  }'
```

### 2. Via Interface Web ADK

1. Acesse http://localhost:8000
2. Crie um novo agente
3. Selecione o modelo on-premise configurado
4. Configure as instru√ß√µes e ferramentas
5. Comece a conversar!

### 3. Via Python

```python
from src.core.llm_factory import LLMFactory
from src.core.llm_provider import LLMMessage

# Obter o provedor para o modelo
provider = LLMFactory.get_provider("nome-do-modelo-onpremise")

# Criar mensagens
messages = [
    LLMMessage(role="system", content="Voc√™ √© um assistente √∫til."),
    LLMMessage(role="user", content="Ol√°, como voc√™ est√°?")
]

# Fazer chat
async for chunk in provider.chat(
    messages=messages,
    model="nome-do-modelo-onpremise",
    temperature=0.1,
    num_predict=500
):
    print(chunk, end="", flush=True)
```

## üîç Detec√ß√£o Autom√°tica de Modelos

O provedor on-premise √© detectado automaticamente quando:

1. **ONPREMISE_MODELS est√° configurado**: Apenas os modelos listados s√£o aceitos
2. **ONPREMISE_MODELS est√° vazio**: Aceita modelos que parecem on-premise:
   - Cont√©m ":" no nome (ex: `gpt-oss:20b`, `llama-2:7b`)
   - Come√ßa com `local-` ou `onpremise-`
   - N√£o s√£o modelos conhecidos do OpenAI ou Gemini

### Exemplos de Nomes de Modelos

‚úÖ **Aceitos como on-premise:**
- `gpt-oss:20b`
- `llama-2:7b`
- `local-model`
- `onpremise-mixtral`
- `custom-model:latest`

‚ùå **Rejeitados (OpenAI/Gemini):**
- `gpt-4o`
- `gpt-3.5-turbo`
- `gemini-2.0-flash-exp`

## üõ†Ô∏è Testando a Configura√ß√£o

Use o script de teste inclu√≠do:

```bash
# Ativar ambiente virtual
source .venv/bin/activate

# Executar teste
python scripts/test_onpremise_provider.py
```

O script ir√°:
1. Verificar as vari√°veis de ambiente
2. Testar a gera√ß√£o de token OAuth
3. Fazer uma requisi√ß√£o de teste ao modelo
4. Exibir o resultado

## ‚ö†Ô∏è Troubleshooting

### Erro: "ONPREMISE_API_BASE_URL not configured"
**Solu√ß√£o**: Configure a vari√°vel `ONPREMISE_API_BASE_URL` no `.env`

### Erro: "Failed to generate OAuth token"
**Solu√ß√µes**:
- Verifique se `ONPREMISE_TOKEN_URL` est√° correto
- Confirme que `ONPREMISE_CONSUMER_KEY` e `ONPREMISE_CONSUMER_SECRET` est√£o corretos
- Para grant type "password", verifique `ONPREMISE_USERNAME` e `ONPREMISE_PASSWORD`

### Erro de Certificado SSL
**Solu√ß√£o**: Adicione `VERIFY_SSL=false` no `.env` (apenas para desenvolvimento!)

### Timeout ou Conex√£o Recusada
**Solu√ß√µes**:
- Verifique se o servidor est√° rodando e acess√≠vel
- Confirme se a URL base est√° correta
- Verifique firewall e pol√≠ticas de rede

### Modelo N√£o Reconhecido
**Solu√ß√£o**: Configure `ONPREMISE_MODELS` com a lista de modelos ou use um nome de modelo com `:` ou prefixo `local-`/`onpremise-`

## üìä Par√¢metros Avan√ßados

### Personalizando Par√¢metros via API

```bash
curl -X POST http://localhost:8001/api/agents/{agent_id}/chat \
  -H "Authorization: Bearer seu_token" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Ol√°!",
    "session_id": "session123",
    "model": "gpt-oss:20b",
    "temperature": 0.5,
    "top_p": 0.9,
    "top_k": 40,
    "num_predict": 1000,
    "repeat_penalty": 1.2,
    "num_ctx": 8192,
    "seed": 42,
    "format": "json"
  }'
```

### Par√¢metros Recomendados

**Para respostas criativas:**
```python
temperature=0.7
top_p=0.9
top_k=40
```

**Para respostas precisas:**
```python
temperature=0.1
top_p=0.15
top_k=0
```

**Para respostas longas:**
```python
num_predict=2000
num_ctx=8192
```

## üîí Seguran√ßa

‚ö†Ô∏è **IMPORTANTE**: 
- Nunca exponha as credenciais OAuth no c√≥digo
- Use sempre `VERIFY_SSL=true` em produ√ß√£o
- Rotacione as credenciais periodicamente
- Use HTTPS para todas as comunica√ß√µes
- Configure timeouts adequados

## üìù Cache de Tokens

O provedor on-premise implementa cache autom√°tico de tokens OAuth:
- Tokens s√£o armazenados em mem√≥ria
- Renova√ß√£o autom√°tica antes da expira√ß√£o
- Margem de seguran√ßa de 60 segundos
- Cache limpo em caso de erro

## üéØ Pr√≥ximos Passos

1. Configure as vari√°veis de ambiente
2. Execute o script de teste
3. Crie um agente usando o modelo on-premise
4. Teste via API ou interface web
5. Ajuste os par√¢metros conforme necess√°rio

## üìö Refer√™ncias

- [Documenta√ß√£o da API On-Premise](https://apidesenv.go.gov.br/docs)
- [OAuth 2.0 Specification](https://oauth.net/2/)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)

