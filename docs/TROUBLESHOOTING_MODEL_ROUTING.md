# Troubleshooting: Roteamento de Modelos

Este documento ajuda a resolver problemas de roteamento de modelos entre diferentes providers.

## üîç Problema: Modelo On-Premise sendo roteado para OpenAI

### Sintoma

Erro ao usar modelo on-premise (ex: `gpt-oss:20b`):
```
Error code: 404 - The model `gpt-oss:20b` does not exist or you do not have access to it.
```

### Causa

O modelo est√° sendo roteado para o provider errado. Por exemplo, `gpt-oss:20b` est√° indo para `OpenAIProvider` em vez de `OnPremiseProvider`.

### Solu√ß√£o

#### 1. Verificar Configura√ß√£o

Certifique-se de que `ONPREMISE_API_BASE_URL` est√° configurado no `.env`:

```env
ONPREMISE_API_BASE_URL=https://apidesenv.go.gov.br/ia/modelos-linguagem-natural/v2.0/
```

#### 2. Verificar Roteamento

Teste qual provider est√° sendo usado:

```python
from src.core.llm_factory import LLMFactory

model = "gpt-oss:20b"
provider = LLMFactory.get_provider(model)
print(f"Provider: {provider.__class__.__name__}")
```

**Esperado:** `OnPremiseProvider`

#### 3. Padr√µes de Nomenclatura

O sistema detecta modelos on-premise pelos seguintes padr√µes:

- ‚úÖ Modelos com `:` (ex: `gpt-oss:20b`, `llama-2:7b`)
- ‚úÖ Modelos com prefixo `local-` (ex: `local-llama`)
- ‚úÖ Modelos com prefixo `onpremise-` (ex: `onpremise-model`)
- ‚úÖ Modelos na lista `ONPREMISE_MODELS` (se configurado)

#### 4. Configurar Lista de Modelos (Opcional)

Se quiser valida√ß√£o antecipada, configure:

```env
ONPREMISE_MODELS=gpt-oss:20b,llama-2:7b,outro-modelo
```

## üìã Como o Roteamento Funciona

### Ordem de Verifica√ß√£o

1. **OnPremiseProvider** (verificado primeiro)
   - Aceita modelos com `:`, `local-`, `onpremise-`
   - Ou modelos na lista `ONPREMISE_MODELS`
   - Rejeita modelos conhecidos OpenAI/Gemini

2. **ADKProvider** (Gemini)
   - Aceita modelos que come√ßam com `gemini-`

3. **OpenAIProvider** (verificado por √∫ltimo)
   - Aceita modelos conhecidos OpenAI
   - Rejeita modelos com `:` (indica on-premise)

### Exemplos

| Modelo | Provider | Motivo |
|--------|----------|--------|
| `gpt-oss:20b` | OnPremise | Tem `:` |
| `gpt-4o` | OpenAI | Modelo conhecido OpenAI |
| `gpt-4o-mini` | OpenAI | Modelo conhecido OpenAI |
| `gemini-2.0-flash-exp` | ADK | Come√ßa com `gemini-` |
| `local-llama` | OnPremise | Prefixo `local-` |
| `llama-2:7b` | OnPremise | Tem `:` |

## ‚úÖ Checklist de Verifica√ß√£o

- [ ] `ONPREMISE_API_BASE_URL` est√° configurado no `.env`
- [ ] `ONPREMISE_TOKEN_URL` est√° configurado (se usar OAuth)
- [ ] `ONPREMISE_CONSUMER_KEY` e `ONPREMISE_CONSUMER_SECRET` est√£o configurados (se usar OAuth)
- [ ] Modelo tem indicador on-premise (`:`, `local-`, `onpremise-`)
- [ ] Provider est√° sendo inicializado (verifique logs)

## üîß Debug

### Verificar Providers Dispon√≠veis

```python
from src.core.llm_factory import LLMFactory

providers = LLMFactory._get_providers()
for provider in providers:
    print(f"Provider: {provider.__class__.__name__}")
```

### Verificar Modelo Espec√≠fico

```python
from src.core.llm_factory import LLMFactory

model = "gpt-oss:20b"
is_supported = LLMFactory.is_model_supported(model)
provider = LLMFactory.get_provider(model)

print(f"Modelo: {model}")
print(f"Suportado: {is_supported}")
print(f"Provider: {provider.__class__.__name__ if provider else 'None'}")
```

### Verificar Todos os Modelos Suportados

```bash
curl http://localhost:8001/api/models
```

## üêõ Problemas Comuns

### Problema 1: "Model not found" mesmo sendo on-premise

**Causa:** Provider on-premise n√£o est√° configurado ou n√£o est√° sendo inicializado.

**Solu√ß√£o:**
1. Verifique se `ONPREMISE_API_BASE_URL` est√° no `.env`
2. Reinicie a aplica√ß√£o
3. Verifique logs para erros de inicializa√ß√£o

### Problema 2: Modelo on-premise indo para OpenAI

**Causa:** Modelo n√£o tem indicador on-premise claro.

**Solu√ß√£o:**
1. Use `:` no nome (ex: `gpt-oss:20b`)
2. Use prefixo `local-` ou `onpremise-`
3. Configure `ONPREMISE_MODELS` com o nome exato

### Problema 3: Modelo OpenAI indo para OnPremise

**Causa:** Nome do modelo confunde o sistema.

**Solu√ß√£o:**
- Use nomes padr√£o OpenAI (ex: `gpt-4o`, `gpt-4o-mini`)
- Evite usar `:` em modelos OpenAI

## üìö Refer√™ncias

- [Configura√ß√£o On-Premise](docs/ONPREMISE_OAUTH_SETUP.md)
- [Modelos Suportados](docs/MULTI_PROVIDER_SETUP.md)

