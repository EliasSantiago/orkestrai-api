# LiteLLM - Gateway Unificado para LLMs

> **üéØ ARQUITETURA SIMPLIFICADA (v2.0 - 2025-11-12)**  
> LiteLLM √© agora o **√öNICO proxy** para todos os modelos LLM.  
> Mais simples. Mais poderoso. Mais recursos.

## üìö √çndice

1. [O que √© LiteLLM?](#o-que-√©-litellm)
2. [Por que usar LiteLLM?](#por-que-usar-litellm)
3. [Arquitetura da Integra√ß√£o](#arquitetura-da-integra√ß√£o)
4. [Configura√ß√£o](#configura√ß√£o)
5. [Guias Espec√≠ficos](#guias-espec√≠ficos)
6. [Refer√™ncias](#refer√™ncias)

## üö® Mudan√ßa Importante

**Nova arquitetura (v2.0)**: LiteLLM √© o √∫nico proxy recomendado.  
**Migra√ß√£o**: Veja [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md)  
**Detalhes t√©cnicos**: Veja [ARCHITECTURE_CHANGE.md](./ARCHITECTURE_CHANGE.md)

---

## O que √© LiteLLM?

**LiteLLM** √© uma biblioteca Python que fornece uma interface unificada para acessar **mais de 100 provedores de modelos LLM**, incluindo:

- ‚úÖ **Google Gemini** (gemini-2.0-flash, gemini-1.5-pro, etc.)
- ‚úÖ **OpenAI** (GPT-4o, GPT-4-turbo, GPT-3.5-turbo, etc.)
- ‚úÖ **Anthropic Claude** (Claude 3 Opus, Sonnet, Haiku)
- ‚úÖ **Ollama** (modelos locais como Llama, Mistral, Gemma)
- ‚úÖ **Azure OpenAI**
- ‚úÖ **Cohere**
- ‚úÖ **HuggingFace**
- ‚úÖ **Replicate**
- ‚úÖ E mais de 90+ outros provedores...

### Caracter√≠sticas Principais

- üîÑ **Interface Unificada**: Use o mesmo c√≥digo para todos os provedores
- üîÅ **Retries Autom√°ticos**: Tenta novamente em caso de falha
- üìä **Rastreamento de Custos**: Monitore quanto voc√™ est√° gastando
- ‚öñÔ∏è **Load Balancing**: Distribua carga entre m√∫ltiplos modelos
- üéØ **Fallback Logic**: Mude para modelo alternativo se o principal falhar
- üìù **Logging & Observability**: Integra√ß√£o com Langfuse, Lunary, MLflow

---

## Por que usar LiteLLM?

### Antes do LiteLLM (Arquitetura Antiga)

```python
# M√∫ltiplos providers, c√≥digo complexo
if model.startswith("gpt"):
    provider = OpenAIProvider()
    response = await provider.chat(...)
elif model.startswith("gemini"):
    provider = ADKProvider()
    response = await provider.chat(...)
elif model.startswith("claude"):
    provider = AnthropicProvider()
    response = await provider.chat(...)
# ... e assim por diante
```

**Problemas:**
- ‚ùå C√≥digo duplicado para cada provider
- ‚ùå Dif√≠cil manuten√ß√£o
- ‚ùå Sem retries autom√°ticos
- ‚ùå Sem load balancing
- ‚ùå Sem fallbacks entre providers

### Depois do LiteLLM (Arquitetura Nova - RECOMENDADA)

```python
# Um √∫nico proxy para TUDO
from src.core.llm_factory import LLMFactory

# O LiteLLM roteia automaticamente para o provider correto
provider = LLMFactory.get_provider("gemini/gemini-2.0-flash-exp")
response = await provider.chat(messages=messages, model="gemini/gemini-2.0-flash-exp")

# Trocar de provider? Apenas mude o nome do modelo!
provider = LLMFactory.get_provider("openai/gpt-4o")
response = await provider.chat(messages=messages, model="openai/gpt-4o")
```

**Vantagens:**
- ‚úÖ Um √∫nico proxy para 100+ providers
- ‚úÖ C√≥digo limpo e unificado
- ‚úÖ Retries autom√°ticos
- ‚úÖ Load balancing configur√°vel
- ‚úÖ Fallbacks entre modelos
- ‚úÖ Cost tracking
- ‚úÖ Observabilidade integrada

### Benef√≠cios para o Projeto

1. **üöÄ Simplicidade**: Um √∫nico provider para gerenciar
2. **üîÑ Flexibilidade**: Troque de modelo/provider sem mudar c√≥digo
3. **üí™ Resili√™ncia**: Retries autom√°ticos e fallbacks configur√°veis
4. **üí∞ Economia**: Compare custos e use modelos mais baratos
5. **‚öñÔ∏è Escalabilidade**: Load balancing entre m√∫ltiplos endpoints
6. **üìä Observabilidade**: Logs, m√©tricas e rastreamento unificados
7. **üõ†Ô∏è Manutenibilidade**: Menos c√≥digo para manter e testar

---

## Arquitetura da Integra√ß√£o

### Estrutura de Arquivos

```
api-adk-google-main/
‚îú‚îÄ‚îÄ litellm_config.yaml              # Configura√ß√£o de modelos e providers
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configura√ß√µes do LiteLLM
‚îÇ   ‚îî‚îÄ‚îÄ core/
‚îÇ       ‚îú‚îÄ‚îÄ llm_factory.py           # Factory que inclui LiteLLMProvider
‚îÇ       ‚îî‚îÄ‚îÄ llm_providers/
‚îÇ           ‚îî‚îÄ‚îÄ litellm_provider.py  # Provider que usa LiteLLM
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ arquitetura/
        ‚îî‚îÄ‚îÄ litellm/                 # Esta documenta√ß√£o
            ‚îú‚îÄ‚îÄ README.md
            ‚îú‚îÄ‚îÄ SETUP.md
            ‚îú‚îÄ‚îÄ CONFIGURATION.md
            ‚îú‚îÄ‚îÄ USAGE.md
            ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
```

### Fluxo de Funcionamento

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Request   ‚îÇ
‚îÇ  (Agent)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LLMFactory     ‚îÇ ‚óÑ‚îÄ‚îÄ Roteia para LiteLLM (√∫nico proxy)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LiteLLMProvider  ‚îÇ ‚óÑ‚îÄ‚îÄ Proxy unificado (√öNICO provider ativo)
‚îÇ   (√öNICO)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LiteLLM       ‚îÇ ‚óÑ‚îÄ‚îÄ Biblioteca que roteia para providers
‚îÇ   (biblioteca)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       v             v             v             v             v
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Gemini ‚îÇ   ‚îÇ OpenAI ‚îÇ   ‚îÇ Claude ‚îÇ   ‚îÇ Ollama ‚îÇ   ‚îÇ +100   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Arquitetura Simplificada

**MUDAN√áA ARQUITETURAL (2025-11-12):**

Agora usamos **APENAS o LiteLLM** como proxy unificado para todos os LLM providers.

#### ‚úÖ Provider √önico

**LiteLLMProvider** - √öNICO proxy para TODOS os modelos
- ‚úÖ Google Gemini (gemini-2.0-flash, gemini-1.5-pro, etc.)
- ‚úÖ OpenAI (GPT-4o, GPT-4-turbo, GPT-3.5-turbo, etc.)
- ‚úÖ Anthropic Claude (Claude 3 Opus, Sonnet, Haiku)
- ‚úÖ Ollama (modelos locais: llama2, mistral, etc.)
- ‚úÖ Azure OpenAI
- ‚úÖ Cohere, HuggingFace, Replicate
- ‚úÖ E mais 90+ outros providers

#### ‚ùå Providers Legados Removidos

Os providers antigos foram **completamente removidos**:
- ~~OnPremiseProvider~~ ‚Üí Use `ollama/` ou custom providers via LiteLLM
- ~~OllamaProvider~~ ‚Üí Use `ollama/modelo` via LiteLLM
- ~~ADKProvider~~ ‚Üí Use `gemini/modelo` via LiteLLM
- ~~OpenAIProvider~~ ‚Üí Use `openai/modelo` via LiteLLM

> **üí° Benef√≠cios da arquitetura √∫nica:**
> - ‚úÖ C√≥digo mais simples e limpo
> - ‚úÖ Interface unificada para todos os providers
> - ‚úÖ Retries autom√°ticos
> - ‚úÖ Load balancing configur√°vel
> - ‚úÖ Fallbacks entre modelos (configur√°vel no YAML)
> - ‚úÖ Cost tracking nativo
> - ‚úÖ Observabilidade integrada

---

## Configura√ß√£o

### 1. Vari√°veis de Ambiente

Adicione ao seu `.env`:

```bash
# Habilitar LiteLLM
LITELLM_ENABLED=true

# API Keys dos providers que voc√™ quer usar
GOOGLE_API_KEY=sua-chave-google
OPENAI_API_KEY=sua-chave-openai
ANTHROPIC_API_KEY=sua-chave-anthropic

# Ollama (se estiver rodando localmente)
OLLAMA_API_BASE_URL=http://localhost:11434

# Configura√ß√µes opcionais
LITELLM_VERBOSE=false          # true para debug
LITELLM_NUM_RETRIES=3          # N√∫mero de tentativas
LITELLM_REQUEST_TIMEOUT=600    # Timeout em segundos (10 min)
```

### 2. Arquivo de Configura√ß√£o

O arquivo `litellm_config.yaml` define os modelos dispon√≠veis:

```yaml
model_list:
  - model_name: gemini-2.0-flash-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
```

### 3. Instala√ß√£o

```bash
# Instalar depend√™ncias (j√° inclu√≠do no requirements.txt)
pip install litellm>=1.50.0

# Ou reinstalar todas as depend√™ncias
pip install -r requirements.txt
```

---

## Guias Espec√≠ficos

Para informa√ß√µes detalhadas sobre cada aspecto do LiteLLM, consulte:

### üìñ [SETUP.md](./SETUP.md)
- Instala√ß√£o passo a passo
- Configura√ß√£o inicial
- Testes de funcionamento

### ‚öôÔ∏è [CONFIGURATION.md](./CONFIGURATION.md)
- Configura√ß√£o avan√ßada
- Customiza√ß√£o de modelos
- Load balancing e fallbacks
- Integra√ß√£o com observability tools

### üíª [USAGE.md](./USAGE.md)
- Como usar no c√≥digo
- Exemplos pr√°ticos
- Cria√ß√£o de agentes com LiteLLM
- Testes e valida√ß√£o

### üîß [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) ‚≠ê NOVO
- Problemas comuns e solu√ß√µes
- **SSL Certificate Verification Error** ‚Üê Solu√ß√£o para erro SSL
- Requested tools not found
- Invalid Model Error
- Connection Timeout
- API Key Invalid
- Debug mode

### üö® [SSL_FIX_GUIDE.md](./SSL_FIX_GUIDE.md) üÜò RESOLVA AGORA
- **Guia r√°pido para corrigir erro SSL** (3 minutos)
- Passo a passo detalhado
- Configura√ß√£o do VERIFY_SSL
- Solu√ß√£o para ambientes corporativos com proxy

### üîÑ [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md) ‚≠ê NOVO
- Guia de migra√ß√£o para arquitetura simplificada
- Convers√£o de modelos legados para formato LiteLLM
- Checklist de migra√ß√£o
- FAQ sobre a mudan√ßa

### üîå [ENDPOINTS_COMPATIBILITY.md](./ENDPOINTS_COMPATIBILITY.md) ‚≠ê NOVO
- Confirma√ß√£o de compatibilidade com endpoints existentes
- O que continua funcionando (tudo!)
- Como testar seus endpoints
- FAQ sobre impacto da mudan√ßa

### üîß [TROUBLESHOOTING.md](./TROUBLESHOOTING.md)
- Problemas comuns e solu√ß√µes
- Debugging
- FAQ

---

## Refer√™ncias

### Documenta√ß√£o Oficial

- **LiteLLM Docs**: https://docs.litellm.ai/docs/
- **LiteLLM GitHub**: https://github.com/BerriAI/litellm
- **Google ADK Docs**: https://google.github.io/adk-docs/

### Documenta√ß√£o Relacionada no Projeto

- [ADDING_NEW_PROVIDER.md](../../ADDING_NEW_PROVIDER.md) - Como adicionar novos providers
- [MULTI_PROVIDER_SETUP.md](../../MULTI_PROVIDER_SETUP.md) - Configura√ß√£o multi-provider
- [OLLAMA_SETUP.md](../../OLLAMA_SETUP.md) - Configura√ß√£o do Ollama

### Links √öteis

- Lista de todos os providers suportados: https://docs.litellm.ai/docs/providers
- Cost tracking: https://docs.litellm.ai/docs/completion/cost_tracking
- Proxy Server: https://docs.litellm.ai/docs/proxy/quick_start

---

## Pr√≥ximos Passos

1. ‚úÖ Leia o [SETUP.md](./SETUP.md) para configurar o LiteLLM
2. ‚úÖ Configure seus providers no `.env`
3. ‚úÖ Teste com exemplos do [USAGE.md](./USAGE.md)
4. ‚úÖ Explore configura√ß√µes avan√ßadas em [CONFIGURATION.md](./CONFIGURATION.md)

---

**√öltima atualiza√ß√£o**: 2025-11-12
**Vers√£o**: 1.0.0

