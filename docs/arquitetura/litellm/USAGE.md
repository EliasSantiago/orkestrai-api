# LiteLLM - Guia de Uso

Este guia mostra como usar o LiteLLM na pr√°tica dentro da aplica√ß√£o.

---

## üìã Sum√°rio

1. [Uso B√°sico](#uso-b√°sico)
2. [Criando Agentes com LiteLLM](#criando-agentes-com-litellm)
3. [Exemplos Pr√°ticos](#exemplos-pr√°ticos)
4. [Nomenclatura de Modelos](#nomenclatura-de-modelos)
5. [Par√¢metros Avan√ßados](#par√¢metros-avan√ßados)
6. [Integra√ß√£o com API](#integra√ß√£o-com-api)

---

## Uso B√°sico

### 1. Usando Diretamente o LiteLLMProvider

```python
import asyncio
from src.core.llm_providers.litellm_provider import LiteLLMProvider
from src.core.llm_provider import LLMMessage

async def exemplo_basico():
    # Criar provider
    provider = LiteLLMProvider()
    
    # Criar mensagens
    messages = [
        LLMMessage(role="user", content="Explique o que √© Python em 2 frases.")
    ]
    
    # Fazer chat (streaming)
    print("Resposta: ", end="")
    async for chunk in provider.chat(
        messages=messages,
        model="gemini/gemini-2.0-flash-exp"
    ):
        print(chunk, end="", flush=True)
    
    print("\n")

# Executar
asyncio.run(exemplo_basico())
```

### 2. Usando o LLMFactory (Recomendado)

```python
import asyncio
from src.core.llm_factory import LLMFactory
from src.core.llm_provider import LLMMessage

async def exemplo_com_factory():
    # O Factory escolhe automaticamente o provider correto
    provider = LLMFactory.get_provider("gemini/gemini-2.0-flash-exp")
    
    if not provider:
        print("Modelo n√£o suportado!")
        return
    
    messages = [
        LLMMessage(role="user", content="Qual √© a capital do Brasil?")
    ]
    
    async for chunk in provider.chat(
        messages=messages,
        model="gemini/gemini-2.0-flash-exp"
    ):
        print(chunk, end="", flush=True)

asyncio.run(exemplo_com_factory())
```

---

## Criando Agentes com LiteLLM

### Exemplo 1: Agente Simples com Gemini via LiteLLM

```python
from src.application.use_cases.agents.create_agent import CreateAgentUseCase
from src.infrastructure.database.agent_repository_impl import AgentRepositoryImpl
from src.domain.services.validation_service import ValidationService
from src.database import SessionLocal

# Criar sess√£o do banco
db = SessionLocal()

try:
    # Criar depend√™ncias
    repository = AgentRepositoryImpl(db)
    validator = ValidationService()
    create_agent = CreateAgentUseCase(repository, validator)
    
    # Criar agente usando modelo via LiteLLM
    agent = create_agent.execute(
        user_id=1,
        name="Assistente Gemini (LiteLLM)",
        description="Assistente que usa Gemini via LiteLLM",
        instruction="Voc√™ √© um assistente √∫til e amig√°vel.",
        model="gemini/gemini-2.0-flash-exp",  # Formato LiteLLM
        tools=[]
    )
    
    print(f"‚úÖ Agente criado: ID {agent.id}")
    print(f"   Nome: {agent.name}")
    print(f"   Modelo: {agent.model}")
    
finally:
    db.close()
```

### Exemplo 2: Agente com OpenAI via LiteLLM

```python
agent = create_agent.execute(
    user_id=1,
    name="Assistente GPT-4o",
    description="Assistente que usa GPT-4o via LiteLLM",
    instruction="Voc√™ √© um expert em programa√ß√£o Python.",
    model="openai/gpt-4o",  # OpenAI via LiteLLM
    tools=[]
)
```

### Exemplo 3: Agente com Ollama Local

```python
agent = create_agent.execute(
    user_id=1,
    name="Assistente Llama Local",
    description="Assistente que usa Llama2 local via Ollama e LiteLLM",
    instruction="Voc√™ √© um assistente offline.",
    model="ollama/llama2",  # Ollama via LiteLLM
    tools=[]
)
```

---

## Exemplos Pr√°ticos

### 1. Chat com Hist√≥rico de Conversa

```python
import asyncio
from src.core.llm_factory import LLMFactory
from src.core.llm_provider import LLMMessage

async def chat_com_historico():
    provider = LLMFactory.get_provider("gemini/gemini-2.0-flash-exp")
    
    # Conversa com m√∫ltiplas mensagens
    messages = [
        LLMMessage(role="user", content="Meu nome √© Jo√£o."),
        LLMMessage(role="assistant", content="Ol√° Jo√£o! Como posso ajudar?"),
        LLMMessage(role="user", content="Qual √© o meu nome?")
    ]
    
    print("Resposta: ")
    async for chunk in provider.chat(
        messages=messages,
        model="gemini/gemini-2.0-flash-exp"
    ):
        print(chunk, end="", flush=True)
    print("\n")

asyncio.run(chat_com_historico())
```

### 2. Usando System Message

```python
async def chat_com_system_message():
    provider = LLMFactory.get_provider("openai/gpt-4o-mini")
    
    messages = [
        LLMMessage(
            role="system", 
            content="Voc√™ √© um poeta. Responda sempre em versos."
        ),
        LLMMessage(role="user", content="Como est√° o dia hoje?")
    ]
    
    async for chunk in provider.chat(
        messages=messages,
        model="openai/gpt-4o-mini"
    ):
        print(chunk, end="", flush=True)

asyncio.run(chat_com_system_message())
```

### 3. Controlando Par√¢metros (temperatura, max_tokens)

```python
async def chat_com_parametros():
    provider = LLMFactory.get_provider("gemini/gemini-2.0-flash-exp")
    
    messages = [
        LLMMessage(role="user", content="Escreva uma hist√≥ria curta sobre um rob√¥.")
    ]
    
    async for chunk in provider.chat(
        messages=messages,
        model="gemini/gemini-2.0-flash-exp",
        temperature=0.9,  # Mais criativo
        max_tokens=500    # Limitar tamanho
    ):
        print(chunk, end="", flush=True)

asyncio.run(chat_com_parametros())
```

### 4. Comparando Respostas de Diferentes Modelos

```python
async def comparar_modelos():
    prompt = "Explique intelig√™ncia artificial em uma frase."
    
    modelos = [
        "gemini/gemini-2.0-flash-exp",
        "openai/gpt-4o-mini",
        "anthropic/claude-3-haiku-20240307",
    ]
    
    for modelo in modelos:
        provider = LLMFactory.get_provider(modelo)
        if not provider:
            continue
        
        print(f"\n{'='*60}")
        print(f"Modelo: {modelo}")
        print('='*60)
        
        messages = [LLMMessage(role="user", content=prompt)]
        
        try:
            async for chunk in provider.chat(messages=messages, model=modelo):
                print(chunk, end="", flush=True)
            print("\n")
        except Exception as e:
            print(f"Erro: {e}")

asyncio.run(comparar_modelos())
```

---

## Nomenclatura de Modelos

O LiteLLM usa o formato `provider/model-name`. Aqui est√£o os principais:

### Google Gemini

```python
"gemini/gemini-2.5-flash"
"gemini/gemini-2.0-flash-exp"
"gemini/gemini-2.0-flash-thinking-exp"
"gemini/gemini-1.5-pro"
"gemini/gemini-1.5-flash"
"gemini/gemini-1.5-flash-8b"
```

### OpenAI

```python
"openai/gpt-4o"
"openai/gpt-4o-mini"
"openai/gpt-4-turbo"
"openai/gpt-4"
"openai/gpt-3.5-turbo"
```

### Anthropic Claude

```python
"anthropic/claude-3-opus-20240229"
"anthropic/claude-3-sonnet-20240229"
"anthropic/claude-3-haiku-20240307"
```

### Ollama (Local)

```python
"ollama/llama2"
"ollama/llama3"
"ollama/mistral"
"ollama/codellama"
"ollama/gemma"
"ollama/phi"
```

### Outros Providers

```python
# Azure OpenAI
"azure/gpt-4o"

# Cohere
"cohere/command-r-plus"

# HuggingFace
"huggingface/meta-llama/Llama-2-7b-chat-hf"

# Replicate
"replicate/meta/llama-2-70b-chat"

# Ver lista completa: https://docs.litellm.ai/docs/providers
```

---

## Par√¢metros Avan√ßados

### Par√¢metros Suportados

```python
async for chunk in provider.chat(
    messages=messages,
    model="gemini/gemini-2.0-flash-exp",
    
    # Controle de gera√ß√£o
    temperature=0.7,        # 0.0 a 1.0 (criatividade)
    max_tokens=2048,        # M√°ximo de tokens na resposta
    top_p=0.9,              # Nucleus sampling
    frequency_penalty=0.0,  # Penalidade por repeti√ß√£o
    presence_penalty=0.0,   # Penalidade por t√≥picos j√° mencionados
    
    # Outros par√¢metros (dependem do provider)
    stop=["END"],           # Tokens de parada
    seed=42,                # Para reprodutibilidade
):
    print(chunk, end="")
```

### Descri√ß√£o dos Par√¢metros

| Par√¢metro | Descri√ß√£o | Faixa | Padr√£o |
|-----------|-----------|-------|---------|
| `temperature` | Controla a aleatoriedade. Maior = mais criativo | 0.0 - 1.0 | 0.7 |
| `max_tokens` | M√°ximo de tokens na resposta | 1 - ‚àû | Varia por modelo |
| `top_p` | Nucleus sampling (alternativa ao temperature) | 0.0 - 1.0 | 1.0 |
| `frequency_penalty` | Penaliza repeti√ß√£o de tokens | -2.0 - 2.0 | 0.0 |
| `presence_penalty` | Penaliza t√≥picos j√° mencionados | -2.0 - 2.0 | 0.0 |

---

## Integra√ß√£o com API

> **‚ö†Ô∏è IMPORTANTE**: O exemplo abaixo √© **apenas did√°tico** para mostrar como usar LiteLLM diretamente.  
> **Sua aplica√ß√£o J√Å TEM endpoints de chat muito superiores!** Veja [ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md)

### Seus Endpoints Existentes (RECOMENDADO)

Voc√™ j√° possui endpoints completos e integrados com LiteLLM:

#### 1. Chat com Agente

```bash
# Endpoint existente: POST /api/agents/chat
curl -X POST http://localhost:8000/api/agents/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "message": "Ol√°, como voc√™ pode me ajudar?",
    "agent_id": 1,
    "session_id": "abc123",
    "model": "gemini/gemini-2.0-flash-exp"
  }'
```

**Features**:
- ‚úÖ Autentica√ß√£o JWT
- ‚úÖ Gest√£o de sess√µes e hist√≥rico
- ‚úÖ Model override
- ‚úÖ Retry logic
- ‚úÖ Tool support
- ‚úÖ File Search/RAG

#### 2. Criar Agente

```bash
# Endpoint existente: POST /api/agents
curl -X POST http://localhost:8000/api/agents \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "name": "Meu Agente",
    "instruction": "Voc√™ √© um assistente",
    "model": "gemini/gemini-2.0-flash-exp"
  }'
```

**Veja mais**: [ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md) - An√°lise completa da sua arquitetura

---

### Exemplo Did√°tico (Apenas para Refer√™ncia)

O exemplo abaixo √© apenas para demonstrar o uso b√°sico do LiteLLM.  
**N√£o √© necess√°rio** implementar este endpoint se voc√™ j√° tem `POST /api/agents/chat`.

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
from src.core.llm_factory import LLMFactory
from src.core.llm_provider import LLMMessage

router = APIRouter()

class ChatRequest(BaseModel):
    model: str
    messages: List[dict]
    temperature: float = 0.7
    max_tokens: int = 2048

@router.post("/chat")
async def chat(request: ChatRequest):
    """Endpoint DID√ÅTICO para chat usando LiteLLM."""
    
    # Obter provider
    provider = LLMFactory.get_provider(request.model)
    if not provider:
        raise HTTPException(
            status_code=400,
            detail=f"Model '{request.model}' is not supported"
        )
    
    # Converter mensagens
    messages = [
        LLMMessage(role=msg["role"], content=msg["content"])
        for msg in request.messages
    ]
    
    # Stream response
    from fastapi.responses import StreamingResponse
    
    async def generate():
        async for chunk in provider.chat(
            messages=messages,
            model=request.model,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        ):
            yield chunk
    
    return StreamingResponse(generate(), media_type="text/plain")
```

> **üí° Nota**: Este √© um exemplo simples. Seu endpoint `POST /api/agents/chat` j√° tem:
> - ‚úÖ Autentica√ß√£o
> - ‚úÖ Gest√£o de conversas
> - ‚úÖ Retry logic
> - ‚úÖ Tool support
> - ‚úÖ File Search/RAG
> - ‚úÖ E muito mais!

---

## üéØ Boas Pr√°ticas

### 1. Tratamento de Erros

```python
async def chat_seguro():
    try:
        provider = LLMFactory.get_provider("gemini/gemini-2.0-flash-exp")
        v
        if not provider:
            print("Modelo n√£o suportado")
            return
        
        messages = [LLMMessage(role="user", content="Ol√°")]
        
        async for chunk in provider.chat(messages=messages, model="gemini/gemini-2.0-flash-exp"):
            print(chunk, end="")
            
    except Exception as e:
        print(f"Erro ao fazer chat: {e}")
        # Log do erro
        import logging
        logging.error(f"Chat error: {e}", exc_info=True)
```

### 2. Verificar Modelo Antes de Usar

```python
def verificar_modelo(model_name: str) -> bool:
    return LLMFactory.is_model_supported(model_name)

# Uso
if verificar_modelo("gemini/gemini-2.0-flash-exp"):
    print("Modelo suportado!")
else:
    print("Modelo n√£o dispon√≠vel")
```

### 3. Listar Modelos Dispon√≠veis

```python
from src.core.llm_factory import LLMFactory

# Listar todos os modelos por provider
all_models = LLMFactory.get_all_supported_models()

for provider_name, models in all_models.items():
    print(f"\n{provider_name}:")
    for model in models:
        print(f"  - {model}")
```

---

## üìö Exemplos Completos

Veja tamb√©m:
- [01_AGENTES_EXEMPLOS_COMPLETOS.md](../../01_AGENTES_EXEMPLOS_COMPLETOS.md)
- [AGENT_CREATION_GUIDE.md](../../AGENT_CREATION_GUIDE.md)

---

**√öltima atualiza√ß√£o**: 2025-11-12

