# LiteLLM Configuration
# This file defines the model routing and configuration for LiteLLM proxy
# Documentation: https://docs.litellm.ai/docs/
#
# IMPORTANTE: Este arquivo é usado para configurações avançadas do LiteLLM (fallbacks, load balancing, etc.)
# A lista de modelos suportados é definida em: src/core/llm_providers/litellm_provider.py
# 
# Para adicionar novos modelos:
# 1. Adicione o modelo em litellm_provider.py (método _get_supported_models)
# 2. Opcionalmente, adicione aqui para configurações avançadas (fallbacks, rate limits, etc.)
#
# O código Python sempre tem precedência para validação de modelos suportados.

model_list:
  # Google Gemini Models (via direct API - Vertex AI credentials are temporarily removed)
  # Latest models (2025)
  - model_name: gemini-3-pro
    litellm_params:
      model: gemini/gemini-3-pro
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GOOGLE_API_KEY
  
  # Experimental models
  - model_name: gemini-2.0-flash-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-2.0-flash-thinking-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-thinking-exp
      api_key: os.environ/GOOGLE_API_KEY
  
  # Stable models
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-1.5-pro-latest
    litellm_params:
      model: gemini/gemini-1.5-pro-latest
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-1.5-flash-8b
    litellm_params:
      model: gemini/gemini-1.5-flash-8b
      api_key: os.environ/GOOGLE_API_KEY
  
  - model_name: gemini-1.0-pro
    litellm_params:
      model: gemini/gemini-1.0-pro
      api_key: os.environ/GOOGLE_API_KEY
  
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY
  
  # Anthropic Claude Models
  - model_name: claude-3-opus-20240229
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
  
  - model_name: claude-3-sonnet-20240229
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
  
  - model_name: claude-3-haiku-20240307
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
  
  - model_name: claude-2.1
    litellm_params:
      model: anthropic/claude-2.1
      api_key: os.environ/ANTHROPIC_API_KEY
  
  - model_name: claude-2
    litellm_params:
      model: anthropic/claude-2
      api_key: os.environ/ANTHROPIC_API_KEY
  
  # Ollama Models (local)
  # Note: Requires Ollama to be running locally
  # Start Ollama: ollama serve
  - model_name: ollama/llama2
    litellm_params:
      model: ollama/llama2
      api_base: os.environ/OLLAMA_API_BASE_URL  # Default: http://localhost:11434
  
  - model_name: ollama/llama3
    litellm_params:
      model: ollama/llama3
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/llama3.1
    litellm_params:
      model: ollama/llama3.1
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/mistral
    litellm_params:
      model: ollama/mistral
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/mixtral
    litellm_params:
      model: ollama/mixtral
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/codellama
    litellm_params:
      model: ollama/codellama
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/gemma
    litellm_params:
      model: ollama/gemma
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/gemma2
    litellm_params:
      model: ollama/gemma2
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/phi
    litellm_params:
      model: ollama/phi
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/phi3
    litellm_params:
      model: ollama/phi3
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/qwen
    litellm_params:
      model: ollama/qwen
      api_base: os.environ/OLLAMA_API_BASE_URL
  
  - model_name: ollama/deepseek-coder
    litellm_params:
      model: ollama/deepseek-coder
      api_base: os.environ/OLLAMA_API_BASE_URL

# General Settings
general_settings:
  # Enable detailed debug logging
  # Set to true for troubleshooting
  set_verbose: false
  
  # Callback settings for logging and monitoring
  # success_callback: ["langfuse", "lunary"]  # Optional: Add observability tools
  
  # Rate limiting (optional)
  # max_parallel_requests: 10
  
  # Timeout settings
  request_timeout: 600  # 10 minutes
  
  # Retry settings
  num_retries: 3
  
  # Cache settings (optional)
  # cache: true
  # cache_params:
  #   type: "redis"
  #   host: "localhost"
  #   port: 6379

# Router settings for load balancing and fallbacks (optional)
# router_settings:
#   routing_strategy: "simple-shuffle"  # Options: simple-shuffle, least-busy, usage-based
#   model_group_alias:
#     gpt-4:
#       - gpt-4o
#       - gpt-4-turbo
#     gemini:
#       - gemini-2.0-flash-exp
#       - gemini-1.5-pro

# Environment variables required:
# - GOOGLE_API_KEY: For Gemini models
# - OPENAI_API_KEY: For OpenAI models
# - ANTHROPIC_API_KEY: For Anthropic Claude models (optional)
# - OLLAMA_API_BASE_URL: For Ollama models (default: http://localhost:11434)
#
# Note: Configure these in your .env file

